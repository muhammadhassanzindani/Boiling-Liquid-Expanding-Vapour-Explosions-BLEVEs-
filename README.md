# Boiling-Liquid-Expanding-Vapour-Explosions-BLEVEs-
Completed Machine Learning project on predictive analysis for the peak pressure generated by (BLEVEs) 2024. The project involved collecting and preprocessing data from various sources, conducting exploratory data analysis, and developing predictive models using algorithms such as Linear Regression, Random Forest, XGBoost, and Neural Networks.  I optimized model performance through hyperparameter tuning and evaluated them using metrics like MAE, RMSE, and R2 scores. The models were interpreted using SHAP values and partial dependency plots. The outcome was a set of highly accurate predictive models, providing valuable insights for improving safety measures in industries handling hazardous materials. The project showcased my proficiency in Python, machine learning tools, and data visualization. 

# Data Preprocessing:
The process began by loading the training and testing datasets using the pandas library. Initial exploration involved checking the shapes of the datasets and identifying any missing values. Column names were standardized by removing spaces and special characters, which is crucial for consistency and ease of manipulation during subsequent steps.

# Exploratory Data Analysis (EDA):
The distribution of key features was analyzed, including "Status," "Sensor Position Side," "Liquid Critical Temperature (K)," "Liquid Boiling Temperature (K)," "Liquid Critical Pressure (bar)," and "Sensor ID." Correlation matrices were generated for both datasets using Pearsonâ€™s method to identify relationships between numerical features, which were visualized with a heatmap to easily spot highly correlated variables.

# Feature Engineering:
One-hot encoding was applied to categorical variables to convert them into a numerical format that machine learning algorithms can process. The columns were reordered, ensuring the target variable, "Target Pressure (bar)," was at the end of the dataset for better accessibility. The log transformation of the target variable was performed to stabilize variance and make the data more normally distributed, which can improve model performance. Outliers in the target variable were identified using the Interquartile Range (IQR) method and were removed to prevent them from skewing the model.

Volumes for the tank and obstacles were calculated based on their dimensions and were also log-transformed. This feature engineering step added additional relevant features that might help in improving the model's accuracy.

# Data Normalization:
The features were normalized using Min-Max Scaling to ensure that all features contribute equally to the model, avoiding dominance by features with larger scales. Histograms were plotted to visually inspect the distribution of the normalized features.

# Model Training and Evaluation:
1. Linear Regression: A linear regression model was initially trained and evaluated on the validation set. The model's performance was measured using Mean Squared Error (MSE), R-squared (R^2), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE).

2. Lasso Regression: LassoCV, a variant of linear regression that includes L1 regularization, was used to identify the most important features by shrinking less important feature coefficients to zero. The model was retrained using only the most important features, and the performance was re-evaluated.

3. Random Forest: A Random Forest Regressor was employed, which is an ensemble learning method that builds multiple decision trees and merges them to get more accurate and stable predictions. The model was evaluated using the same set of metrics.

4. Neural Network: A Multi-Layer Perceptron (MLP) neural network model was also trained. Hyperparameter tuning was performed using GridSearchCV, testing different configurations of hidden layer sizes, activation functions, and regularization strengths. The best model was selected based on cross-validation results.

# Predictions:
Finally, predictions were made on the test set using both the Lasso Regression and Random Forest models. The predictions were transformed back from the logarithmic scale to the original scale and saved to CSV files for submission or further analysis.

In conclusion, this project involved a thorough approach to understanding and preparing the data, applying various regression models, and fine-tuning them to achieve the best possible prediction accuracy. The use of multiple models and techniques ensured that the most effective features were used, and that the models could generalize well to unseen data.
